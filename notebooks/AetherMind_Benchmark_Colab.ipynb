{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2498cf79",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q httpx aiohttp datasets litellm tqdm nest_asyncio python-dotenv\n",
    "\n",
    "# Enable nested asyncio for Colab\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2288bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# =============================================================================\n",
    "# üîë CONFIGURE YOUR API KEY HERE\n",
    "# =============================================================================\n",
    "# Option 1: Set directly (not recommended for shared notebooks)\n",
    "# AETHER_API_KEY = \"am_live_your_key_here\"\n",
    "\n",
    "# Option 2: Use Colab secrets (recommended) - Add key in left sidebar -> üîë Secrets\n",
    "try:\n",
    "    AETHER_API_KEY = userdata.get('AETHER_API_KEY')\n",
    "except:\n",
    "    AETHER_API_KEY = None\n",
    "\n",
    "# Option 3: Environment variable\n",
    "if not AETHER_API_KEY:\n",
    "    AETHER_API_KEY = os.getenv('AETHER_API_KEY', os.getenv('AETHERMIND_API_KEY'))\n",
    "\n",
    "# =============================================================================\n",
    "# üåê API CONFIGURATION\n",
    "# =============================================================================\n",
    "PRODUCTION_API = \"https://aetheragi.onrender.com\"\n",
    "LOCAL_API = \"http://localhost:8000\"\n",
    "\n",
    "# Choose which API to use\n",
    "API_BASE = PRODUCTION_API  # Change to LOCAL_API for local testing\n",
    "\n",
    "# =============================================================================\n",
    "# üìä BENCHMARK SETTINGS - FULL DATASET BY DEFAULT\n",
    "# =============================================================================\n",
    "# 0 = ALL questions (full benchmark for best results)\n",
    "# Set to a number like 20 or 50 for quick testing\n",
    "QUESTIONS_PER_FAMILY = 0  # üéØ FULL DATASET for official benchmark results!\n",
    "\n",
    "MAX_CONCURRENT_FAMILIES = 8   # Parallel benchmark families\n",
    "MAX_CONCURRENT_QUESTIONS = 10 # Parallel API calls per family\n",
    "TIMEOUT_SECONDS = 180  # Per-question timeout (increased for full dataset)\n",
    "\n",
    "# Dataset sizes (for reference):\n",
    "# GSM8K:       1,319 test questions\n",
    "# MMLU:       14,042 test questions\n",
    "# ARC:         1,172 test questions\n",
    "# HellaSwag:  10,042 validation questions\n",
    "# WinoGrande:  1,267 validation questions\n",
    "# TruthfulQA:    817 validation questions\n",
    "# TOTAL:     ~28,659 questions\n",
    "\n",
    "print(f\"üåê API Endpoint: {API_BASE}\")\n",
    "print(f\"üîë API Key: {'‚úÖ Configured' if AETHER_API_KEY else '‚ö†Ô∏è Not set (will use unauthenticated mode)'}\")\n",
    "print(f\"üìä Questions per family: {'ALL (full dataset)' if QUESTIONS_PER_FAMILY == 0 else QUESTIONS_PER_FAMILY}\")\n",
    "print(f\"üîÑ Max concurrent families: {MAX_CONCURRENT_FAMILIES}\")\n",
    "print(f\"‚ö° Max concurrent questions: {MAX_CONCURRENT_QUESTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3ffe5",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Benchmark Client & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbda512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from tqdm.asyncio import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "class BenchmarkType(Enum):\n",
    "    MATH_REASONING = \"math_reasoning\"\n",
    "    KNOWLEDGE = \"knowledge\"\n",
    "    CODING = \"coding\"\n",
    "    LOGICAL_REASONING = \"logical_reasoning\"\n",
    "    LANGUAGE = \"language\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkFamily:\n",
    "    \"\"\"Configuration for a benchmark family.\"\"\"\n",
    "    name: str\n",
    "    benchmark_type: BenchmarkType\n",
    "    description: str\n",
    "    answer_format: str  # \"number\", \"letter\", \"code\", \"text\"\n",
    "    answer_regex: Optional[str] = None\n",
    "    dataset_source: Optional[str] = None\n",
    "    hf_subset: Optional[str] = None  # For datasets with subsets like MMLU\n",
    "\n",
    "\n",
    "# All benchmark families to run\n",
    "BENCHMARK_FAMILIES = {\n",
    "    \"gsm8k\": BenchmarkFamily(\n",
    "        name=\"GSM-8K\",\n",
    "        benchmark_type=BenchmarkType.MATH_REASONING,\n",
    "        description=\"Grade school math word problems\",\n",
    "        answer_format=\"number\",\n",
    "        answer_regex=r\"(?:####\\s*)?(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\",\n",
    "        dataset_source=\"gsm8k\",\n",
    "        hf_subset=\"main\",\n",
    "    ),\n",
    "    \"mmlu\": BenchmarkFamily(\n",
    "        name=\"MMLU\",\n",
    "        benchmark_type=BenchmarkType.KNOWLEDGE,\n",
    "        description=\"Massive Multitask Language Understanding\",\n",
    "        answer_format=\"letter\",\n",
    "        answer_regex=r\"(?:^|\\s)([A-D])(?:\\s|$|\\.|,)\",\n",
    "        dataset_source=\"cais/mmlu\",\n",
    "        hf_subset=\"all\",\n",
    "    ),\n",
    "    \"arc_challenge\": BenchmarkFamily(\n",
    "        name=\"ARC-Challenge\",\n",
    "        benchmark_type=BenchmarkType.LOGICAL_REASONING,\n",
    "        description=\"AI2 Reasoning Challenge\",\n",
    "        answer_format=\"letter\",\n",
    "        answer_regex=r\"(?:^|\\s)([A-D])(?:\\s|$|\\.|,)\",\n",
    "        dataset_source=\"allenai/ai2_arc\",\n",
    "        hf_subset=\"ARC-Challenge\",\n",
    "    ),\n",
    "    \"hellaswag\": BenchmarkFamily(\n",
    "        name=\"HellaSwag\",\n",
    "        benchmark_type=BenchmarkType.LOGICAL_REASONING,\n",
    "        description=\"Commonsense reasoning\",\n",
    "        answer_format=\"letter\",\n",
    "        answer_regex=r\"(?:^|\\s)([A-D])(?:\\s|$|\\.|,)\",\n",
    "        dataset_source=\"Rowan/hellaswag\",\n",
    "    ),\n",
    "    \"winogrande\": BenchmarkFamily(\n",
    "        name=\"WinoGrande\",\n",
    "        benchmark_type=BenchmarkType.LANGUAGE,\n",
    "        description=\"Pronoun resolution\",\n",
    "        answer_format=\"number\",\n",
    "        answer_regex=r\"([12])\",\n",
    "        dataset_source=\"winogrande\",\n",
    "        hf_subset=\"winogrande_xl\",\n",
    "    ),\n",
    "    \"truthfulqa\": BenchmarkFamily(\n",
    "        name=\"TruthfulQA\",\n",
    "        benchmark_type=BenchmarkType.KNOWLEDGE,\n",
    "        description=\"Questions to test truthfulness\",\n",
    "        answer_format=\"letter\",\n",
    "        answer_regex=r\"(?:^|\\s)([A-D])(?:\\s|$|\\.|,)\",\n",
    "        dataset_source=\"truthful_qa\",\n",
    "        hf_subset=\"multiple_choice\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"üìö Loaded {len(BENCHMARK_FAMILIES)} benchmark families:\")\n",
    "for name, family in BENCHMARK_FAMILIES.items():\n",
    "    print(f\"   ‚Ä¢ {family.name}: {family.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fdc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark mode system prompt (same as local)\n",
    "BENCHMARK_SYSTEM_PROMPT = \"\"\"You are being evaluated on a benchmark test. \n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Output ONLY your final answer - no explanations, no reasoning, no tags\n",
    "2. Do NOT use any XML tags like <think>, <aether-write>, etc.\n",
    "3. Do NOT explain your work - just give the answer\n",
    "4. Do NOT say \"I think\" or \"The answer is\" - just output the answer itself\n",
    "\n",
    "ANSWER FORMAT: {format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "FORMAT_INSTRUCTIONS = {\n",
    "    \"number\": \"Output only the numerical answer (e.g., 42 or -15.5)\",\n",
    "    \"letter\": \"Output only the letter (A, B, C, or D)\",\n",
    "    \"code\": \"Output only Python code inside ```python``` blocks\",\n",
    "    \"text\": \"Output only the answer text, no explanations\",\n",
    "}\n",
    "\n",
    "\n",
    "class AetherBenchmarkClient:\n",
    "    \"\"\"Async client for calling AetherAGI API in benchmark mode.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_base: str, api_key: Optional[str] = None):\n",
    "        self.api_base = api_base.rstrip('/')\n",
    "        self.api_key = api_key\n",
    "        self._client: Optional[httpx.AsyncClient] = None\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        self._client = httpx.AsyncClient(timeout=httpx.Timeout(TIMEOUT_SECONDS))\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, *args):\n",
    "        if self._client:\n",
    "            await self._client.aclose()\n",
    "    \n",
    "    def _build_system_prompt(self, answer_format: str) -> str:\n",
    "        return BENCHMARK_SYSTEM_PROMPT.format(\n",
    "            format_instructions=FORMAT_INSTRUCTIONS.get(answer_format, \"Output only your answer.\")\n",
    "        )\n",
    "    \n",
    "    def _strip_tags(self, response: str) -> str:\n",
    "        \"\"\"Remove any XML tags from response.\"\"\"\n",
    "        if not response:\n",
    "            return \"\"\n",
    "        response = re.sub(r'<aether-[^>]*>.*?</aether-[^>]*>', '', response, flags=re.DOTALL)\n",
    "        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "        return response.strip()\n",
    "    \n",
    "    async def ask(self, question: str, answer_format: str = \"text\") -> Dict[str, Any]:\n",
    "        \"\"\"Ask a benchmark question via the API.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        if self.api_key:\n",
    "            headers[\"X-Aether-Key\"] = self.api_key\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"aethermind-v1\",\n",
    "            \"user\": \"colab_benchmark_runner\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self._build_system_prompt(answer_format)},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            \"metadata\": {\n",
    "                \"benchmark_mode\": True,\n",
    "                \"answer_format\": answer_format,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = await self._client.post(\n",
    "                f\"{self.api_base}/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            raw_response = data[\"choices\"][0][\"message\"][\"content\"] or \"\"\n",
    "            \n",
    "            return {\n",
    "                \"response\": self._strip_tags(raw_response),\n",
    "                \"raw_response\": raw_response,\n",
    "                \"latency_ms\": (time.time() - start_time) * 1000,\n",
    "                \"tokens_used\": data.get(\"usage\", {}).get(\"total_tokens\", 0),\n",
    "                \"error\": None,\n",
    "            }\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            return {\n",
    "                \"response\": \"\",\n",
    "                \"raw_response\": f\"HTTP Error: {e.response.status_code}\",\n",
    "                \"latency_ms\": (time.time() - start_time) * 1000,\n",
    "                \"tokens_used\": 0,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"response\": \"\",\n",
    "                \"raw_response\": f\"Error: {str(e)}\",\n",
    "                \"latency_ms\": (time.time() - start_time) * 1000,\n",
    "                \"tokens_used\": 0,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "print(\"‚úÖ AetherBenchmarkClient ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541592b",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f315e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "@dataclass\n",
    "class Question:\n",
    "    \"\"\"A benchmark question.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    correct_answer: str\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "def load_gsm8k_questions(num_samples: int) -> List[Question]:\n",
    "    \"\"\"Load GSM8K math questions. num_samples=0 loads ALL questions.\"\"\"\n",
    "    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "    if num_samples <= 0:\n",
    "        samples = list(ds)\n",
    "        print(f\"   üì• GSM8K: Loading ALL {len(samples)} questions...\")\n",
    "    else:\n",
    "        samples = list(ds.shuffle(seed=42).select(range(min(num_samples, len(ds)))))\n",
    "    \n",
    "    questions = []\n",
    "    for i, item in enumerate(samples):\n",
    "        answer_text = item[\"answer\"]\n",
    "        match = re.search(r\"####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\", answer_text)\n",
    "        correct = match.group(1).replace(\",\", \"\") if match else \"0\"\n",
    "        \n",
    "        questions.append(Question(\n",
    "            id=f\"gsm8k_{i}\",\n",
    "            text=item[\"question\"],\n",
    "            correct_answer=correct,\n",
    "        ))\n",
    "    return questions\n",
    "\n",
    "\n",
    "def load_mmlu_questions(num_samples: int) -> List[Question]:\n",
    "    \"\"\"Load MMLU knowledge questions. num_samples=0 loads ALL questions.\"\"\"\n",
    "    ds = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")\n",
    "    if num_samples <= 0:\n",
    "        samples = list(ds)\n",
    "        print(f\"   üì• MMLU: Loading ALL {len(samples)} questions...\")\n",
    "    else:\n",
    "        samples = list(ds.shuffle(seed=42).select(range(min(num_samples, len(ds)))))\n",
    "    \n",
    "    questions = []\n",
    "    for i, item in enumerate(samples):\n",
    "        choices = item[\"choices\"]\n",
    "        formatted = f\"{item['question']}\\n\\n\"\n",
    "        for j, choice in enumerate(choices):\n",
    "            formatted += f\"{chr(65+j)}) {choice}\\n\"\n",
    "        \n",
    "        correct_idx = item[\"answer\"]\n",
    "        correct_letter = chr(65 + correct_idx) if isinstance(correct_idx, int) else correct_idx\n",
    "        \n",
    "        questions.append(Question(\n",
    "            id=f\"mmlu_{i}\",\n",
    "            text=formatted,\n",
    "            correct_answer=correct_letter,\n",
    "            metadata={\"subject\": item.get(\"subject\", \"unknown\")},\n",
    "        ))\n",
    "    return questions\n",
    "\n",
    "\n",
    "def load_arc_questions(num_samples: int) -> List[Question]:\n",
    "    \"\"\"Load ARC-Challenge questions. num_samples=0 loads ALL questions.\"\"\"\n",
    "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "    if num_samples <= 0:\n",
    "        samples = list(ds)\n",
    "        print(f\"   üì• ARC-Challenge: Loading ALL {len(samples)} questions...\")\n",
    "    else:\n",
    "        samples = list(ds.shuffle(seed=42).select(range(min(num_samples, len(ds)))))\n",
    "    \n",
    "    questions = []\n",
    "    for i, item in enumerate(samples):\n",
    "        choices = item[\"choices\"]\n",
    "        formatted = f\"{item['question']}\\n\\n\"\n",
    "        for label, text in zip(choices[\"label\"], choices[\"text\"]):\n",
    "            formatted += f\"{label}) {text}\\n\"\n",
    "        \n",
    "        questions.append(Question(\n",
    "            id=f\"arc_{i}\",\n",
    "            text=formatted,\n",
    "            correct_answer=item[\"answerKey\"],\n",
    "        ))\n",
    "    return questions\n",
    "\n",
    "\n",
    "def load_hellaswag_questions(num_samples: int) -> List[Question]:\n",
    "    \"\"\"Load HellaSwag commonsense questions. num_samples=0 loads ALL questions.\"\"\"\n",
    "    ds = load_dataset(\"Rowan/hellaswag\", split=\"validation\")\n",
    "    if num_samples <= 0:\n",
    "        samples = list(ds)\n",
    "        print(f\"   üì• HellaSwag: Loading ALL {len(samples)} questions...\")\n",
    "    else:\n",
    "        samples = list(ds.shuffle(seed=42).select(range(min(num_samples, len(ds)))))\n",
    "    \n",
    "    questions = []\n",
    "    for i, item in enumerate(samples):\n",
    "        context = item[\"ctx\"]\n",
    "        endings = item[\"endings\"]\n",
    "        \n",
    "        formatted = f\"Complete the following:\\n\\n{context}\\n\\n\"\n",
    "        for j, ending in enumerate(endings):\n",
    "            formatted += f\"{chr(65+j)}) {ending}\\n\"\n",
    "        \n",
    "        correct_idx = int(item[\"label\"])\n",
    "        \n",
    "        questions.append(Question(\n",
    "            id=f\"hellaswag_{i}\",\n",
    "            text=formatted,\n",
    "            correct_answer=chr(65 + correct_idx),\n",
    "        ))\n",
    "    return questions\n",
    "\n",
    "\n",
    "def load_winogrande_questions(num_samples: int) -> List[Question]:\n",
    "    \"\"\"Load WinoGrande pronoun resolution questions. num_samples=0 loads ALL questions.\"\"\"\n",
    "    ds = load_dataset(\"winogrande\", \"winogrande_xl\", split=\"validation\")\n",
    "    if num_samples <= 0:\n",
    "        samples = list(ds)\n",
    "        print(f\"   üì• WinoGrande: Loading ALL {len(samples)} questions...\")\n",
    "    else:\n",
    "        samples = list(ds.shuffle(seed=42).select(range(min(num_samples, len(ds)))))\n",
    "    \n",
    "    questions = []\n",
    "    for i, item in enumerate(samples):\n",
    "        sentence = item[\"sentence\"]\n",
    "        opt1 = item[\"option1\"]\n",
    "        opt2 = item[\"option2\"]\n",
    "        \n",
    "        formatted = f\"{sentence}\\n\\nWhich option fits best in the blank?\\n1) {opt1}\\n2) {opt2}\\n\\nAnswer with 1 or 2.\"\n",
    "        \n",
    "        questions.append(Question(\n",
    "            id=f\"winogrande_{i}\",\n",
    "            text=formatted,\n",
    "            correct_answer=item[\"answer\"],\n",
    "        ))\n",
    "    return questions\n",
    "\n",
    "\n",
    "def load_truthfulqa_questions(num_samples: int) -> List[Question]:\n",
    "    \"\"\"Load TruthfulQA questions. num_samples=0 loads ALL questions.\"\"\"\n",
    "    ds = load_dataset(\"truthful_qa\", \"multiple_choice\", split=\"validation\")\n",
    "    if num_samples <= 0:\n",
    "        samples = list(ds)\n",
    "        print(f\"   üì• TruthfulQA: Loading ALL {len(samples)} questions...\")\n",
    "    else:\n",
    "        samples = list(ds.shuffle(seed=42).select(range(min(num_samples, len(ds)))))\n",
    "    \n",
    "    questions = []\n",
    "    for i, item in enumerate(samples):\n",
    "        q = item[\"question\"]\n",
    "        choices = item[\"mc1_targets\"][\"choices\"]\n",
    "        labels = item[\"mc1_targets\"][\"labels\"]\n",
    "        \n",
    "        formatted = f\"{q}\\n\\n\"\n",
    "        correct_letter = \"A\"\n",
    "        for j, (choice, label) in enumerate(zip(choices[:4], labels[:4])):\n",
    "            formatted += f\"{chr(65+j)}) {choice}\\n\"\n",
    "            if label == 1:\n",
    "                correct_letter = chr(65+j)\n",
    "        \n",
    "        questions.append(Question(\n",
    "            id=f\"truthfulqa_{i}\",\n",
    "            text=formatted,\n",
    "            correct_answer=correct_letter,\n",
    "        ))\n",
    "    return questions\n",
    "\n",
    "\n",
    "# Map family names to loaders\n",
    "DATASET_LOADERS = {\n",
    "    \"gsm8k\": load_gsm8k_questions,\n",
    "    \"mmlu\": load_mmlu_questions,\n",
    "    \"arc_challenge\": load_arc_questions,\n",
    "    \"hellaswag\": load_hellaswag_questions,\n",
    "    \"winogrande\": load_winogrande_questions,\n",
    "    \"truthfulqa\": load_truthfulqa_questions,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Dataset loaders ready!\")\n",
    "print(\"üìä Full dataset sizes:\")\n",
    "print(\"   ‚Ä¢ GSM8K:       1,319 questions\")\n",
    "print(\"   ‚Ä¢ MMLU:       14,042 questions\") \n",
    "print(\"   ‚Ä¢ ARC:         1,172 questions\")\n",
    "print(\"   ‚Ä¢ HellaSwag:  10,042 questions\")\n",
    "print(\"   ‚Ä¢ WinoGrande:  1,267 questions\")\n",
    "print(\"   ‚Ä¢ TruthfulQA:    817 questions\")\n",
    "print(\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(\"   ‚Ä¢ TOTAL:     ~28,659 questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71522329",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Answer Checking & Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ae8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response: str, answer_format: str, answer_regex: Optional[str]) -> str:\n",
    "    \"\"\"Extract the answer from model response.\"\"\"\n",
    "    if not response:\n",
    "        return \"\"\n",
    "    \n",
    "    response = response.strip()\n",
    "    \n",
    "    if answer_regex:\n",
    "        matches = re.findall(answer_regex, response, re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches[-1].strip()  # Take last match (final answer)\n",
    "    \n",
    "    # Fallback extraction\n",
    "    if answer_format == \"letter\":\n",
    "        # Look for standalone letter\n",
    "        match = re.search(r\"\\b([A-D])\\b\", response.upper())\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    elif answer_format == \"number\":\n",
    "        # Look for number\n",
    "        match = re.search(r\"(-?\\d+(?:\\.\\d+)?)\", response.replace(\",\", \"\"))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    return response.split()[0] if response.split() else \"\"\n",
    "\n",
    "\n",
    "def check_answer(extracted: str, correct: str, answer_format: str) -> bool:\n",
    "    \"\"\"Check if extracted answer matches correct answer.\"\"\"\n",
    "    if not extracted or not correct:\n",
    "        return False\n",
    "    \n",
    "    extracted = extracted.strip().upper()\n",
    "    correct = correct.strip().upper()\n",
    "    \n",
    "    if answer_format == \"number\":\n",
    "        try:\n",
    "            ext_num = float(extracted.replace(\",\", \"\"))\n",
    "            cor_num = float(correct.replace(\",\", \"\"))\n",
    "            return abs(ext_num - cor_num) < 0.01  # Fuzzy match for numbers\n",
    "        except:\n",
    "            return extracted == correct\n",
    "    \n",
    "    return extracted == correct\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FamilyResult:\n",
    "    \"\"\"Results for one benchmark family.\"\"\"\n",
    "    family_name: str\n",
    "    total_questions: int\n",
    "    correct: int\n",
    "    score: float\n",
    "    avg_latency_ms: float\n",
    "    total_tokens: int\n",
    "    errors: int\n",
    "    details: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"family\": self.family_name,\n",
    "            \"total\": self.total_questions,\n",
    "            \"correct\": self.correct,\n",
    "            \"score\": f\"{self.score*100:.1f}%\",\n",
    "            \"avg_latency_ms\": f\"{self.avg_latency_ms:.0f}\",\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"errors\": self.errors,\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Answer checking ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773b195",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Concurrent Benchmark Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_single_family(\n",
    "    client: AetherBenchmarkClient,\n",
    "    family_name: str,\n",
    "    family: BenchmarkFamily,\n",
    "    questions: List[Question],\n",
    "    semaphore: asyncio.Semaphore,\n",
    ") -> FamilyResult:\n",
    "    \"\"\"Run a single benchmark family.\"\"\"\n",
    "    \n",
    "    correct = 0\n",
    "    total_latency = 0.0\n",
    "    total_tokens = 0\n",
    "    errors = 0\n",
    "    details = []\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting {family.name} ({len(questions)} questions)...\")\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        async with semaphore:  # Limit concurrent API calls\n",
    "            result = await client.ask(question.text, family.answer_format)\n",
    "        \n",
    "        if result[\"error\"]:\n",
    "            errors += 1\n",
    "            extracted = \"\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            extracted = extract_answer(result[\"response\"], family.answer_format, family.answer_regex)\n",
    "            is_correct = check_answer(extracted, question.correct_answer, family.answer_format)\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        \n",
    "        total_latency += result[\"latency_ms\"]\n",
    "        total_tokens += result[\"tokens_used\"]\n",
    "        \n",
    "        details.append({\n",
    "            \"question_id\": question.id,\n",
    "            \"correct_answer\": question.correct_answer,\n",
    "            \"extracted\": extracted,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"latency_ms\": result[\"latency_ms\"],\n",
    "        })\n",
    "        \n",
    "        # Progress indicator\n",
    "        status = \"‚úì\" if is_correct else \"‚úó\"\n",
    "        print(f\"   [{family_name}] {i+1}/{len(questions)} {status}\", end=\"\\r\")\n",
    "    \n",
    "    score = correct / len(questions) if questions else 0\n",
    "    avg_latency = total_latency / len(questions) if questions else 0\n",
    "    \n",
    "    print(f\"\\n‚úÖ {family.name}: {correct}/{len(questions)} ({score*100:.1f}%)\")\n",
    "    \n",
    "    return FamilyResult(\n",
    "        family_name=family_name,\n",
    "        total_questions=len(questions),\n",
    "        correct=correct,\n",
    "        score=score,\n",
    "        avg_latency_ms=avg_latency,\n",
    "        total_tokens=total_tokens,\n",
    "        errors=errors,\n",
    "        details=details,\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_all_benchmarks_concurrent(\n",
    "    api_base: str,\n",
    "    api_key: Optional[str],\n",
    "    families: Dict[str, BenchmarkFamily],\n",
    "    questions_per_family: int,\n",
    "    max_concurrent: int = 4,\n",
    ") -> Dict[str, FamilyResult]:\n",
    "    \"\"\"\n",
    "    Run ALL benchmark families concurrently.\n",
    "    \n",
    "    Args:\n",
    "        api_base: API endpoint URL\n",
    "        api_key: Optional API key\n",
    "        families: Dict of benchmark families to run\n",
    "        questions_per_family: Number of questions per family\n",
    "        max_concurrent: Max concurrent API calls (rate limiting)\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping family name to FamilyResult\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"üß† AetherMind Concurrent Benchmark Runner\")\n",
    "    print(f\"üåê API: {api_base}\")\n",
    "    print(f\"üìä Families: {len(families)}\")\n",
    "    print(f\"‚ùì Questions per family: {questions_per_family}\")\n",
    "    print(f\"üîÑ Max concurrent calls: {max_concurrent}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load all datasets first\n",
    "    print(\"\\nüì• Loading datasets...\")\n",
    "    family_questions = {}\n",
    "    for name, family in families.items():\n",
    "        if name in DATASET_LOADERS:\n",
    "            try:\n",
    "                questions = DATASET_LOADERS[name](questions_per_family)\n",
    "                family_questions[name] = questions\n",
    "                print(f\"   ‚úÖ {family.name}: {len(questions)} questions loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {family.name}: Failed to load - {e}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {family.name}: No loader available\")\n",
    "    \n",
    "    # Semaphore for rate limiting\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    # Run all families concurrently\n",
    "    print(\"\\nüèÉ Running benchmarks concurrently...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    async with AetherBenchmarkClient(api_base, api_key) as client:\n",
    "        tasks = []\n",
    "        for name, questions in family_questions.items():\n",
    "            family = families[name]\n",
    "            task = run_single_family(client, name, family, questions, semaphore)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        results_list = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Process results\n",
    "    results = {}\n",
    "    for name, result in zip(family_questions.keys(), results_list):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"‚ùå {name} failed: {result}\")\n",
    "        else:\n",
    "            results[name] = result\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_correct = sum(r.correct for r in results.values())\n",
    "    total_questions = sum(r.total_questions for r in results.values())\n",
    "    overall_score = total_correct / total_questions if total_questions else 0\n",
    "    \n",
    "    print(f\"\\n{'Family':<20} {'Score':<12} {'Correct':<12} {'Latency':<12}\")\n",
    "    print(\"-\"*56)\n",
    "    for name, result in sorted(results.items(), key=lambda x: x[1].score, reverse=True):\n",
    "        print(f\"{result.family_name:<20} {result.score*100:>6.1f}%     {result.correct:>3}/{result.total_questions:<3}       {result.avg_latency_ms:>6.0f}ms\")\n",
    "    \n",
    "    print(\"-\"*56)\n",
    "    print(f\"{'OVERALL':<20} {overall_score*100:>6.1f}%     {total_correct:>3}/{total_questions:<3}\")\n",
    "    print(f\"\\n‚è±Ô∏è Total time: {total_time:.1f}s\")\n",
    "    print(f\"üìÖ Timestamp: {datetime.now(timezone.utc).isoformat()}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Concurrent runner ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6e809",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick health check on the API\n",
    "import httpx\n",
    "\n",
    "async def test_api_connection():\n",
    "    print(f\"üîç Testing connection to {API_BASE}...\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        try:\n",
    "            # Try health endpoint first\n",
    "            response = await client.get(f\"{API_BASE}/health\")\n",
    "            if response.status_code == 200:\n",
    "                print(f\"‚úÖ API is healthy!\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            # Try a simple chat completion\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            if AETHER_API_KEY:\n",
    "                headers[\"X-Aether-Key\"] = AETHER_API_KEY\n",
    "            \n",
    "            response = await client.post(\n",
    "                f\"{API_BASE}/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                json={\n",
    "                    \"model\": \"aethermind-v1\",\n",
    "                    \"user\": \"connection_test\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": \"Say 'OK' and nothing else.\"}],\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                reply = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                print(f\"‚úÖ API responded: {reply[:50]}...\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è API returned status {response.status_code}\")\n",
    "                print(f\"   Response: {response.text[:200]}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Connection failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Run the test\n",
    "api_ok = asyncio.get_event_loop().run_until_complete(test_api_connection())\n",
    "\n",
    "if not api_ok:\n",
    "    print(\"\\nüí° Troubleshooting tips:\")\n",
    "    print(\"   1. Check if the API is running at the specified URL\")\n",
    "    print(\"   2. Verify your API key is correct\")\n",
    "    print(\"   3. The Render service may be sleeping - try again in 30s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a3c6c",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Run All Benchmarks! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0483e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÉ RUN ALL BENCHMARK FAMILIES CONCURRENTLY!\n",
    "\n",
    "# Select which families to run (comment out any you want to skip)\n",
    "FAMILIES_TO_RUN = {\n",
    "    \"gsm8k\": BENCHMARK_FAMILIES[\"gsm8k\"],\n",
    "    \"mmlu\": BENCHMARK_FAMILIES[\"mmlu\"],\n",
    "    \"arc_challenge\": BENCHMARK_FAMILIES[\"arc_challenge\"],\n",
    "    \"hellaswag\": BENCHMARK_FAMILIES[\"hellaswag\"],\n",
    "    \"winogrande\": BENCHMARK_FAMILIES[\"winogrande\"],\n",
    "    \"truthfulqa\": BENCHMARK_FAMILIES[\"truthfulqa\"],\n",
    "}\n",
    "\n",
    "# Run the benchmarks!\n",
    "results = asyncio.get_event_loop().run_until_complete(\n",
    "    run_all_benchmarks_concurrent(\n",
    "        api_base=API_BASE,\n",
    "        api_key=AETHER_API_KEY,\n",
    "        families=FAMILIES_TO_RUN,\n",
    "        questions_per_family=QUESTIONS_PER_FAMILY,\n",
    "        max_concurrent=MAX_CONCURRENT_FAMILIES,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8e377",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60771134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "output = {\n",
    "    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"api_endpoint\": API_BASE,\n",
    "    \"questions_per_family\": QUESTIONS_PER_FAMILY,\n",
    "    \"results\": {name: r.to_dict() for name, r in results.items()},\n",
    "    \"overall_score\": sum(r.correct for r in results.values()) / sum(r.total_questions for r in results.values()) if results else 0,\n",
    "}\n",
    "\n",
    "# Save locally\n",
    "filename = f\"aethermind_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {filename}\")\n",
    "\n",
    "# Download link for Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    print(\"üì• Download started!\")\n",
    "except:\n",
    "    print(\"   (Run in Colab to enable auto-download)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7725c03",
   "metadata": {},
   "source": [
    "## üìä Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c48342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if results:\n",
    "    families = list(results.keys())\n",
    "    scores = [results[f].score * 100 for f in families]\n",
    "    names = [results[f].family_name for f in families]\n",
    "    \n",
    "    # Sort by score\n",
    "    sorted_data = sorted(zip(names, scores), key=lambda x: x[1], reverse=True)\n",
    "    names, scores = zip(*sorted_data)\n",
    "    \n",
    "    # Color based on score\n",
    "    colors = ['#2ecc71' if s >= 70 else '#f39c12' if s >= 50 else '#e74c3c' for s in scores]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.barh(names, scores, color=colors)\n",
    "    plt.xlabel('Score (%)')\n",
    "    plt.title('üß† AetherMind Benchmark Results')\n",
    "    plt.xlim(0, 100)\n",
    "    \n",
    "    # Add score labels\n",
    "    for bar, score in zip(bars, scores):\n",
    "        plt.text(score + 1, bar.get_y() + bar.get_height()/2, f'{score:.1f}%', \n",
    "                 va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('benchmark_results.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìà Chart saved to: benchmark_results.png\")\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3cfd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Custom Benchmark Run\n",
    "\n",
    "Use this cell to run specific families with custom settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom single-family run\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# custom_results = asyncio.get_event_loop().run_until_complete(\n",
    "#     run_all_benchmarks_concurrent(\n",
    "#         api_base=API_BASE,\n",
    "#         api_key=AETHER_API_KEY,\n",
    "#         families={\"gsm8k\": BENCHMARK_FAMILIES[\"gsm8k\"]},  # Single family\n",
    "#         questions_per_family=100,  # More questions\n",
    "#         max_concurrent=2,  # Lower concurrency for stability\n",
    "#     )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
